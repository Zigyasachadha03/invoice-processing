{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cd42aed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Dense, Flatten, Dropout, BatchNormalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e6caaacc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set data and label paths\n",
    "data_path = 'C:\\\\Users\\\\zigya\\\\OneDrive\\\\Desktop\\\\Invoice-2\\\\data\\\\images'\n",
    "label_path = 'C:\\\\Users\\\\zigya\\\\OneDrive\\\\Desktop\\\\Invoice-2\\\\data\\\\labels'\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cab54acc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load images and labels\n",
    "images = []\n",
    "labels =[]\n",
    "for filename in os.listdir(data_path):\n",
    "    img = cv2.imread(os.path.join(data_path, filename))\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    img = cv2.resize(img, (256, 256))\n",
    "    images.append(img)\n",
    "    label_file = os.path.join(label_path, os.path.splitext(filename)[0] + '.xml')\n",
    "    with open(label_file, 'r') as f:\n",
    "        label_data = f.read()\n",
    "        label_data = label_data.strip()\n",
    "        labels.append(label_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "02162072",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert labels to one-hot encoding\n",
    "label_classes = sorted(list(set(labels)))\n",
    "label_to_idx = {label: i for i, label in enumerate(label_classes)}\n",
    "idx_to_label = {i: label for i, label in enumerate(label_classes)}\n",
    "labels = [label_to_idx[label] for label in labels]\n",
    "labels = to_categorical(labels, num_classes=len(label_classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "967f7f03",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open('idx_to_label.json', 'w') as f:\n",
    "    json.dump(idx_to_label, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5528bb5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233]\n"
     ]
    }
   ],
   "source": [
    "print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "52835c51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "234"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d7926b9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into training, validation, and test sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(images, labels, test_size=0.2, random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_val, y_val, test_size=0.5, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "23d2f688",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Augment data\n",
    "datagen_train = ImageDataGenerator(\n",
    "    rotation_range=10,\n",
    "    width_shift_range=0.1,\n",
    "    height_shift_range=0.1,\n",
    "    shear_range=0.1,\n",
    "    zoom_range=0.1,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode='nearest')\n",
    "\n",
    "datagen_val = ImageDataGenerator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "46815c3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert data\n",
    "X_train = np.array(X_train)\n",
    "X_val = np.array(X_val)\n",
    "X_test = np.array(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6f7b7cad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape data for Conv2D layer\n",
    "X_train = X_train.reshape(X_train.shape[0], X_train.shape[1], X_train.shape[2], 1)\n",
    "X_val = X_val.reshape(X_val.shape[0], X_val.shape[1], X_val.shape[2], 1)\n",
    "X_test = X_test.reshape(X_test.shape[0], X_test.shape[1], X_test.shape[2], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a4371bc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build model\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, (3, 3), activation='relu', padding='same', input_shape=(256, 256, 1)))\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv2D(64, (3, 3), activation='relu', padding='same'))\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv2D(128, (3, 3), activation='relu', padding='same'))\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv2D(256, (3, 3), activation='relu', padding='same'))\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv2D(512, (3, 3), activation='relu', padding='same'))\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Flatten())\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(len(label_classes), activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c267c32b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define callbacks\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=5, verbose=1, mode='min')\n",
    "model_checkpoint = ModelCheckpoint('best_model.h5', monitor='val_loss', mode='min', save_best_only=True, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "618bd941",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "5/5 [==============================] - ETA: 0s - loss: 9.4651 - accuracy: 0.0063  \n",
      "Epoch 1: val_loss improved from inf to 154.33902, saving model to best_model.h5\n",
      "5/5 [==============================] - 47s 7s/step - loss: 9.4651 - accuracy: 0.0063 - val_loss: 154.3390 - val_accuracy: 0.0000e+00\n",
      "Epoch 2/50\n",
      "5/5 [==============================] - ETA: 0s - loss: 8.3260 - accuracy: 0.0000e+00\n",
      "Epoch 2: val_loss did not improve from 154.33902\n",
      "5/5 [==============================] - 28s 6s/step - loss: 8.3260 - accuracy: 0.0000e+00 - val_loss: 396.4248 - val_accuracy: 0.0000e+00\n",
      "Epoch 3/50\n",
      "5/5 [==============================] - ETA: 0s - loss: 6.1322 - accuracy: 0.0063 \n",
      "Epoch 3: val_loss did not improve from 154.33902\n",
      "5/5 [==============================] - 28s 5s/step - loss: 6.1322 - accuracy: 0.0063 - val_loss: 660.7739 - val_accuracy: 0.0000e+00\n",
      "Epoch 4/50\n",
      "5/5 [==============================] - ETA: 0s - loss: 5.7035 - accuracy: 0.0063  \n",
      "Epoch 4: val_loss did not improve from 154.33902\n",
      "5/5 [==============================] - 27s 6s/step - loss: 5.7035 - accuracy: 0.0063 - val_loss: 706.9528 - val_accuracy: 0.0000e+00\n",
      "Epoch 5/50\n",
      "5/5 [==============================] - ETA: 0s - loss: 5.4851 - accuracy: 0.0000e+00\n",
      "Epoch 5: val_loss did not improve from 154.33902\n",
      "5/5 [==============================] - 30s 6s/step - loss: 5.4851 - accuracy: 0.0000e+00 - val_loss: 558.6392 - val_accuracy: 0.0000e+00\n",
      "Epoch 6/50\n",
      "5/5 [==============================] - ETA: 0s - loss: 5.4370 - accuracy: 0.0000e+00\n",
      "Epoch 6: val_loss did not improve from 154.33902\n",
      "5/5 [==============================] - 28s 6s/step - loss: 5.4370 - accuracy: 0.0000e+00 - val_loss: 427.1700 - val_accuracy: 0.0000e+00\n",
      "Epoch 6: early stopping\n"
     ]
    }
   ],
   "source": [
    "# Train model\n",
    "batch_size = 32\n",
    "epochs = 50\n",
    "history = model.fit(datagen_train.flow(X_train, y_train, batch_size=batch_size), \n",
    "                    epochs=epochs, \n",
    "                    batch_size=batch_size, \n",
    "                    validation_data=datagen_val.flow(X_val, y_val, batch_size=batch_size), \n",
    "                    callbacks=[early_stop, model_checkpoint])\n",
    "# history = model.fit(datagen_train.flow(X_train, y_train, batch_size=batch_size),\n",
    "# validation_data=datagen_val.flow(X_val, y_val, batch_size=batch_size),\n",
    "# epochs=epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ba602cd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 - 2s - loss: 429.9192 - accuracy: 0.0000e+00 - 2s/epoch - 2s/step\n",
      "Test Accuracy: 0.0\n"
     ]
    }
   ],
   "source": [
    "# Evaluate model\n",
    "test_loss, test_acc = model.evaluate(X_test, y_test, verbose=2)\n",
    "print('Test Accuracy:', test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7e7922c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('test_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "90b29371",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "loaded_model = load_model('test_model.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "62664a7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "invoice_image = cv2.imread('test images/invoice-254.jpg')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bab06d77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess the image\n",
    "img_gray = cv2.cvtColor(invoice_image, cv2.COLOR_BGR2GRAY)\n",
    "img_resized = cv2.resize(img_gray, (256, 256))\n",
    "img_reshaped = img_resized.reshape(1, 256, 256, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "73e88d7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 4s 4s/step\n"
     ]
    }
   ],
   "source": [
    "# Get the model's predictions for the input image\n",
    "pred_probs = loaded_model.predict(img_reshaped)[0]\n",
    "\n",
    "# Get the predicted class label\n",
    "pred_class = np.argmax(pred_probs)\n",
    "\n",
    "# Map the predicted class label back to the original class name\n",
    "pred_label = idx_to_label[pred_class]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f2282ee3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<annotation>\\n\\t<folder>output</folder>\\n\\t<filename>invoice-101.jpg</filename>\\n\\t<path>C:\\\\Users\\\\zigya\\\\OneDrive\\\\Desktop\\\\Label_Img Text Detect\\\\data\\\\images\\\\output\\\\invoice-101.jpg</path>\\n\\t<source>\\n\\t\\t<database>Unknown</database>\\n\\t</source>\\n\\t<size>\\n\\t\\t<width>1654</width>\\n\\t\\t<height>2339</height>\\n\\t\\t<depth>3</depth>\\n\\t</size>\\n\\t<segmented>0</segmented>\\n\\t<object>\\n\\t\\t<name>Company Name</name>\\n\\t\\t<pose>Unspecified</pose>\\n\\t\\t<truncated>0</truncated>\\n\\t\\t<difficult>0</difficult>\\n\\t\\t<bndbox>\\n\\t\\t\\t<xmin>125</xmin>\\n\\t\\t\\t<ymin>3</ymin>\\n\\t\\t\\t<xmax>613</xmax>\\n\\t\\t\\t<ymax>106</ymax>\\n\\t\\t</bndbox>\\n\\t</object>\\n\\t<object>\\n\\t\\t<name>Name</name>\\n\\t\\t<pose>Unspecified</pose>\\n\\t\\t<truncated>0</truncated>\\n\\t\\t<difficult>0</difficult>\\n\\t\\t<bndbox>\\n\\t\\t\\t<xmin>1320</xmin>\\n\\t\\t\\t<ymin>643</ymin>\\n\\t\\t\\t<xmax>1555</xmax>\\n\\t\\t\\t<ymax>683</ymax>\\n\\t\\t</bndbox>\\n\\t</object>\\n\\t<object>\\n\\t\\t<name>Address</name>\\n\\t\\t<pose>Unspecified</pose>\\n\\t\\t<truncated>0</truncated>\\n\\t\\t<difficult>0</difficult>\\n\\t\\t<bndbox>\\n\\t\\t\\t<xmin>1050</xmin>\\n\\t\\t\\t<ymin>679</ymin>\\n\\t\\t\\t<xmax>1563</xmax>\\n\\t\\t\\t<ymax>788</ymax>\\n\\t\\t</bndbox>\\n\\t</object>\\n\\t<object>\\n\\t\\t<name>Item Description</name>\\n\\t\\t<pose>Unspecified</pose>\\n\\t\\t<truncated>0</truncated>\\n\\t\\t<difficult>0</difficult>\\n\\t\\t<bndbox>\\n\\t\\t\\t<xmin>160</xmin>\\n\\t\\t\\t<ymin>1144</ymin>\\n\\t\\t\\t<xmax>941</xmax>\\n\\t\\t\\t<ymax>1326</ymax>\\n\\t\\t</bndbox>\\n\\t</object>\\n\\t<object>\\n\\t\\t<name>Item Amount</name>\\n\\t\\t<pose>Unspecified</pose>\\n\\t\\t<truncated>0</truncated>\\n\\t\\t<difficult>0</difficult>\\n\\t\\t<bndbox>\\n\\t\\t\\t<xmin>1444</xmin>\\n\\t\\t\\t<ymin>1210</ymin>\\n\\t\\t\\t<xmax>1532</xmax>\\n\\t\\t\\t<ymax>1255</ymax>\\n\\t\\t</bndbox>\\n\\t</object>\\n\\t<object>\\n\\t\\t<name>Total Amount</name>\\n\\t\\t<pose>Unspecified</pose>\\n\\t\\t<truncated>0</truncated>\\n\\t\\t<difficult>0</difficult>\\n\\t\\t<bndbox>\\n\\t\\t\\t<xmin>1440</xmin>\\n\\t\\t\\t<ymin>1372</ymin>\\n\\t\\t\\t<xmax>1534</xmax>\\n\\t\\t\\t<ymax>1408</ymax>\\n\\t\\t</bndbox>\\n\\t</object>\\n</annotation>'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cd90f921",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytesseract\n",
    "import xml.etree.ElementTree as ET\n",
    "def get_annotation_info(xml_file_path, img_path):\n",
    "    #function to get the annotation details from the XML file and extract the text using OCR\n",
    "    \n",
    "    root = ET.fromstring(xml_file_path)\n",
    "    \n",
    "    annotations = [] #list to store the annotations\n",
    "    img = cv2.imread(img_path) #reading the input image\n",
    "    \n",
    "    norm_img = np.zeros((img.shape[0], img.shape[1]))\n",
    "    img = cv2.normalize(img, norm_img, 0, 255, cv2.NORM_MINMAX)\n",
    "    \n",
    "    # noise = cv2.fastNlMeansDenoisingColored(img, None, 10, 10, 7, 15)\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY) #converting the image to grayscale\n",
    "    thresh = cv2.threshold(gray, 64, 255, cv2.THRESH_BINARY_INV + cv2.THRESH_OTSU)[1] #applying threshold to the image\n",
    "    for obj in root.findall('object'):\n",
    "        # Extract the label name\n",
    "        annotation = {}\n",
    "        annotation['label'] = obj.find('name').text\n",
    "\n",
    "        # Extract the bounding box coordinates\n",
    "        bbox = obj.find('bndbox')\n",
    "        xmin = int(bbox.find('xmin').text)\n",
    "        ymin = int(bbox.find('ymin').text)\n",
    "        xmax = int(bbox.find('xmax').text)\n",
    "        ymax = int(bbox.find('ymax').text)\n",
    "        cropped_img = thresh[ymin:ymax, xmin:xmax] #cropping the image based on the bounding box coordinates\n",
    "        annotation['text'] = pytesseract.image_to_string(cropped_img) #using OCR to extract the text from the cropped image\n",
    "        annotations.append(annotation) #adding the annotation to the list\n",
    "        \n",
    "        \n",
    "    return annotations #returning the list of annotations\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e4c2c5ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "annotations = get_annotation_info(pred_label , 'test images/invoice-254.jpg')\n",
    "\n",
    "invoice_data = {}\n",
    "\n",
    "# Loop through the annotations and add the information to the dictionary\n",
    "for annotation in annotations:\n",
    "    if(annotation['text'] == \"\"): \n",
    "        continue\n",
    "    else:\n",
    "        label = annotation['label']\n",
    "        text = annotation['text']\n",
    "        invoice_data[label] = text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "09b54909",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Company Name': 'amazon.in\\n',\n",
       " 'Name': 'ANANT BHAT\\n',\n",
       " 'Address': '-aridabad, Gali number 2 sector 11\\nFARIDABAD, HARYANA, 121006\\nIN\\n',\n",
       " 'Item Description': 'NIVEA Men Face Wash, Oil Control for 12hr Oil Control with 10x\\nVitamin C Effect, 150 mi | BOOX9UOCE! ( BOOXSUOCEI )\\nHSN:33041000\\n\\nTAL:\\n'}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "invoice_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3177264f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import os\n",
    "\n",
    "# Path of the folder to save the file in\n",
    "folder_path = 'C:\\\\Users\\\\zigya\\\\OneDrive\\\\Desktop\\\\Invoice-2\\\\static\\\\uploaded_files\\\\'\n",
    "\n",
    "# Path of the file to save\n",
    "file = os.path.join(folder_path, 'output_2.csv')\n",
    "\n",
    "def create_csv(invoice_data, file_path):\n",
    "    with open(file_path ,'w', newline='') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow(['Label', 'Text'])\n",
    "        for key, value in invoice_data.items():\n",
    "            writer.writerow([key, value])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "765bf47e",
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_data = create_csv(invoice_data, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "29bc419a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "\n",
      "    \"Company Name\": \"amazon.in\\n\",\n",
      "\n",
      "    \"Name\": \"ANANT BHAT\\n\",\n",
      "\n",
      "    \"Address\": \"-aridabad,\n",
      " Gali number 2 sector 11\\nFARIDABAD,\n",
      " HARYANA,\n",
      " 121006\\nIN\\n\",\n",
      "\n",
      "    \"Item Description\": \"NIVEA Men Face Wash,\n",
      " Oil Control for 12hr Oil Control with 10x\\nVitamin C Effect,\n",
      " 150 mi | BOOX9UOCE! ( BOOXSUOCEI )\\nHSN:33041000\\n\\nTAL:\\n\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "# Convert the dictionary to a formatted JSON string with newlines and indentation\n",
    "invoice_data_str = json.dumps(invoice_data, indent=4)\n",
    "\n",
    "# Add a newline after each opening curly brace and comma in the JSON string\n",
    "invoice_data_str = invoice_data_str.replace(\"{\", \"{\\n\").replace(\",\", \",\\n\")\n",
    "\n",
    "# Print the formatted JSON string\n",
    "print(invoice_data_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3a8ad19b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "# Path of the folder to save the file in\n",
    "folder_path = 'C:\\\\Users\\\\zigya\\\\OneDrive\\\\Desktop\\\\Invoice-2\\\\'\n",
    "\n",
    "# Path of the file to save\n",
    "filename = datetime.now().strftime(\"%Y%m%d_%H%M%S_\") + 'invoice_data'\n",
    "file_path = os.path.join(folder_path, filename + '.csv')\n",
    "\n",
    "# Write the data to the CSV file\n",
    "with open(file_path, 'w', newline='') as csv_file:\n",
    "    writer = csv.writer(csv_file)\n",
    "    for key, value in invoice_data.items():\n",
    "        writer.writerow([key, value.strip()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32711599",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
